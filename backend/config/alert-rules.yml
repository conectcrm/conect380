# Prometheus Alert Rules - ConectCRM
# Regras de alerta críticas para monitoramento de produção

groups:
  # ========================================
  # GRUPO 1: Disponibilidade do Sistema
  # ========================================
  - name: system_availability
    interval: 30s
    rules:
      # Alerta 1: API está fora do ar
      - alert: APIDown
        expr: up{job="conectcrm-backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "API ConectCRM está fora do ar"
          description: "A instância {{ $labels.instance }} não está respondendo há {{ $value }} minutos"
          runbook: "https://docs.conectcrm.com/runbooks/api-down"
          impact: "Clientes não conseguem acessar o sistema"
          action: "Verificar logs da aplicação e reiniciar se necessário"

      # Alerta 2: Database connection pool esgotado
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          (typeorm_connection_pool_active / typeorm_connection_pool_max) > 0.9
        for: 5m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Pool de conexões PostgreSQL quase esgotado"
          description: "{{ $value | humanizePercentage }} das conexões em uso"
          runbook: "https://docs.conectcrm.com/runbooks/db-pool-exhausted"

      # Alerta 3: Taxa de erro HTTP > 5%
      - alert: HighHTTPErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) 
            / 
            sum(rate(http_requests_total[5m]))
          ) > 0.05
        for: 3m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Taxa de erros HTTP elevada"
          description: "{{ $value | humanizePercentage }} de requisições falhando"

  # ========================================
  # GRUPO 2: Performance & Latência
  # ========================================
  - name: performance
    interval: 30s
    rules:
      # Alerta 4: Latência P95 > 2s
      - alert: HighLatencyP95
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, method, route)
          ) > 2
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Latência P95 acima do SLO"
          description: "Rota {{ $labels.route }} com P95 de {{ $value }}s"
          slo: "Latência P95 < 2s"

      # Alerta 5: Latência P99 > 5s
      - alert: HighLatencyP99
        expr: |
          histogram_quantile(0.99, 
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, method, route)
          ) > 5
        for: 3m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "Latência P99 crítica"
          description: "Rota {{ $labels.route }} com P99 de {{ $value }}s"

      # Alerta 6: Tempo de query no banco > 1s
      - alert: SlowDatabaseQueries
        expr: |
          histogram_quantile(0.95,
            sum(rate(typeorm_query_duration_seconds_bucket[5m])) by (le, entity)
          ) > 1
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Queries lentas detectadas"
          description: "Entity {{ $labels.entity }} com P95 de {{ $value }}s"

  # ========================================
  # GRUPO 3: Recursos do Sistema
  # ========================================
  - name: system_resources
    interval: 30s
    rules:
      # Alerta 7: CPU > 80%
      - alert: HighCPUUsage
        expr: |
          (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Uso de CPU elevado"
          description: "CPU em {{ $value }}% de uso na instância {{ $labels.instance }}"

      # Alerta 8: Memória > 85%
      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Uso de memória elevado"
          description: "Memória em {{ $value }}% de uso"

      # Alerta 9: Disco > 90%
      - alert: DiskSpaceRunningOut
        expr: |
          (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Espaço em disco crítico"
          description: "Disco {{ $labels.mountpoint }} com {{ $value }}% de uso"
          action: "Limpar logs antigos ou expandir volume"

  # ========================================
  # GRUPO 4: Atendimento & Tickets
  # ========================================
  - name: atendimento
    interval: 1m
    rules:
      # Alerta 10: Tickets na fila > 50
      - alert: HighTicketQueueSize
        expr: |
          tickets_em_fila_total > 50
        for: 10m
        labels:
          severity: warning
          component: atendimento
        annotations:
          summary: "Fila de tickets elevada"
          description: "{{ $value }} tickets aguardando atendimento"

      # Alerta 11: Tempo médio de resposta > 30min
      - alert: SlowTicketResponseTime
        expr: |
          avg(ticket_tempo_primeira_resposta_seconds) / 60 > 30
        for: 15m
        labels:
          severity: warning
          component: atendimento
        annotations:
          summary: "Tempo de resposta elevado"
          description: "Tempo médio de {{ $value }} minutos"
          slo: "Primeira resposta < 30min"

      # Alerta 12: Taxa de abandono > 15%
      - alert: HighTicketAbandonmentRate
        expr: |
          (
            sum(rate(tickets_abandonados_total[1h])) 
            / 
            sum(rate(tickets_criados_total[1h]))
          ) > 0.15
        for: 30m
        labels:
          severity: critical
          component: atendimento
        annotations:
          summary: "Taxa de abandono crítica"
          description: "{{ $value | humanizePercentage }} dos tickets abandonados"

  # ========================================
  # GRUPO 5: SLOs (Service Level Objectives)
  # ========================================
  - name: slos
    interval: 1m
    rules:
      # SLO 1: Disponibilidade 99.9% (monthly)
      - alert: SLOAvailabilityViolation
        expr: |
          (
            1 - (
              sum(rate(http_requests_total{status=~"5.."}[30d]))
              /
              sum(rate(http_requests_total[30d]))
            )
          ) < 0.999
        labels:
          severity: critical
          type: slo
          slo_name: availability
          window: 30d
        annotations:
          summary: "SLO de Disponibilidade violado"
          description: "Disponibilidade atual: {{ $value | humanizePercentage }}"
          target: "99.9%"

      # SLO 2: Latência P95 < 2s (weekly)
      - alert: SLOLatencyViolation
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[7d])) by (le)
          ) > 2
        labels:
          severity: warning
          type: slo
          slo_name: latency_p95
          window: 7d
        annotations:
          summary: "SLO de Latência P95 violado"
          description: "P95 atual: {{ $value }}s"
          target: "< 2s"

      # SLO 3: Error Budget consumido > 80%
      - alert: ErrorBudgetExhausted
        expr: |
          (
            sum(increase(http_requests_total{status=~"5.."}[30d]))
            /
            (sum(increase(http_requests_total[30d])) * 0.001)
          ) > 0.8
        for: 1h
        labels:
          severity: critical
          type: slo
          slo_name: error_budget
          window: 30d
        annotations:
          summary: "Error Budget quase esgotado"
          description: "{{ $value | humanizePercentage }} do budget consumido"
          action: "Congelar deploys e focar em estabilidade"

  # ========================================
  # GRUPO 6: Business Metrics
  # ========================================
  - name: business_metrics
    interval: 5m
    rules:
      # Alerta 13: Queda brusca de tráfego
      - alert: TrafficDropDetected
        expr: |
          (
            sum(rate(http_requests_total[5m]))
            /
            sum(rate(http_requests_total[5m] offset 1h))
          ) < 0.5
        for: 10m
        labels:
          severity: warning
          component: business
        annotations:
          summary: "Queda significativa de tráfego"
          description: "Tráfego {{ $value | humanizePercentage }} menor que 1h atrás"

      # Alerta 14: Taxa de conversão baixa
      - alert: LowConversionRate
        expr: |
          (
            sum(rate(tickets_resolvidos_total[1h]))
            /
            sum(rate(tickets_criados_total[1h]))
          ) < 0.5
        for: 2h
        labels:
          severity: info
          component: business
        annotations:
          summary: "Taxa de conversão abaixo do esperado"
          description: "Apenas {{ $value | humanizePercentage }} dos tickets resolvidos"

  # ========================================
  # GRUPO 7: Dashboard V2 Benchmark Gates
  # ========================================
  - name: dashboard_v2_benchmark
    interval: 1m
    rules:
      # Alerta 15: Benchmark sem execucao recente (>36h)
      - alert: DashboardV2BenchmarkStale
        expr: |
          time() - max(push_time_seconds{job="dashboard_v2_benchmark"}) > 129600
        for: 10m
        labels:
          severity: warning
          component: dashboard_v2
          type: performance_gate
        annotations:
          summary: "Benchmark Dashboard V2 sem atualizacao recente"
          description: "Sem push no job dashboard_v2_benchmark ha mais de 36h"
          action: "Verificar workflow dashboard-v2-benchmark no GitHub Actions"

      # Alerta 16: Gate de p95 falhou
      - alert: DashboardV2BenchmarkP95GateFailed
        expr: |
          max(conectcrm_dashboard_v2_benchmark_gate_pass{gate="cache_hit_p95"}) < 1
        for: 5m
        labels:
          severity: critical
          component: dashboard_v2
          type: performance_gate
        annotations:
          summary: "Gate P95 do Dashboard V2 falhou"
          description: "Benchmark reportou cache-hit p95 acima do limite de 400ms"
          action: "Investigar regressao de consulta/caching em /dashboard/v2/*"

      # Alerta 17: Gate cold-start falhou
      - alert: DashboardV2BenchmarkColdGateFailed
        expr: |
          max(conectcrm_dashboard_v2_benchmark_gate_pass{gate="cold"}) < 1
        for: 5m
        labels:
          severity: critical
          component: dashboard_v2
          type: performance_gate
        annotations:
          summary: "Gate cold-start do Dashboard V2 falhou"
          description: "Benchmark reportou cold-start acima do limite de 1200ms"
          action: "Verificar agregacoes, queries e processamento de warmup"

      # Alerta 18: Benchmark retornou erros HTTP
      - alert: DashboardV2BenchmarkHttpErrors
        expr: |
          max(conectcrm_dashboard_v2_benchmark_non2xx_total) > 0
        for: 5m
        labels:
          severity: warning
          component: dashboard_v2
          type: performance_gate
        annotations:
          summary: "Benchmark Dashboard V2 encontrou respostas nao-2xx"
          description: "Foram detectadas respostas HTTP nao-2xx no ultimo benchmark"
          action: "Revisar rate-limit, auth e disponibilidade das rotas /dashboard/v2/*"

  # ========================================
  # GRUPO 8: Financeiro - Monitor de Alertas Operacionais
  # ========================================
  - name: financeiro_alertas_monitor
    interval: 1m
    rules:
      # Alerta 19: Monitor sem ciclo recente
      - alert: FinanceiroAlertasMonitorStaleCycle
        expr: |
          time() - conectcrm_financeiro_alertas_monitor_ultimo_ciclo_timestamp_seconds > 900
        for: 5m
        labels:
          severity: critical
          component: financeiro
          type: monitor
        annotations:
          summary: "Monitor de alertas financeiros sem ciclo recente"
          description: "Nao ha ciclo concluido do monitor financeiro ha mais de 15 minutos"
          action: "Verificar logs do backend e status do monitor automatico"

      # Alerta 20: Ciclo fatal do monitor
      - alert: FinanceiroAlertasMonitorFatalCycle
        expr: |
          sum(increase(conectcrm_financeiro_alertas_monitor_ciclos_total{status="fatal_error"}[15m])) >= 1
        for: 1m
        labels:
          severity: critical
          component: financeiro
          type: monitor
        annotations:
          summary: "Monitor de alertas financeiros com ciclo fatal"
          description: "Foi detectado ao menos um ciclo fatal do monitor nos ultimos 15 minutos"
          action: "Investigar falha global de recalculo de alertas financeiros"

      # Alerta 21: Taxa de falhas/parciais elevada
      - alert: FinanceiroAlertasMonitorHighFailureRate
        expr: |
          (
            sum(increase(conectcrm_financeiro_alertas_monitor_ciclos_total{status=~"partial|fatal_error"}[30m]))
            /
            clamp_min(sum(increase(conectcrm_financeiro_alertas_monitor_ciclos_total[30m])), 1)
          ) > 0.2
        for: 10m
        labels:
          severity: warning
          component: financeiro
          type: monitor
        annotations:
          summary: "Taxa de falha do monitor financeiro acima do limite"
          description: "Mais de 20% dos ciclos do monitor financeiro estao em partial/fatal nos ultimos 30 minutos"
          action: "Analisar empresas com erro recorrente e capacidade de processamento"

      # Alerta 22: Duracao do ciclo acima do baseline esperado
      - alert: FinanceiroAlertasMonitorSlowCycle
        expr: |
          conectcrm_financeiro_alertas_monitor_ultimo_ciclo_duracao_segundos > 120
        for: 10m
        labels:
          severity: warning
          component: financeiro
          type: monitor
        annotations:
          summary: "Ciclo do monitor financeiro lento"
          description: "Duracao do ultimo ciclo acima de 120 segundos"
          action: "Validar volume de dados e performance das queries do motor de alertas"

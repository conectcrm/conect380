# âš™ï¸ CalibraÃ§Ã£o de Thresholds - Alerting

**Objetivo**: Ajustar sensibilidade de alertas baseado em dados histÃ³ricos reais.  
**Quando usar**: ApÃ³s 1-2 semanas de dados coletados OU se houver muitos falsos positivos.

---

## ğŸ¯ Por Que Calibrar?

**Problema**: Thresholds padrÃ£o podem nÃ£o se adequar ao seu perfil de uso:
- âŒ **Falsos positivos**: Alertas disparam sem problema real (fadiga de alerta)
- âŒ **Falsos negativos**: Problema real nÃ£o dispara alerta (detecÃ§Ã£o tardia)

**SoluÃ§Ã£o**: Analisar dados histÃ³ricos e ajustar thresholds baseado em:
- ğŸ“Š Percentis reais (P50, P95, P99)
- ğŸ“ˆ PadrÃµes sazonais (horÃ¡rio comercial vs madrugada)
- ğŸ¯ Taxa de falsos positivos/negativos atual

---

## ğŸ“Š AnÃ¡lise de Dados HistÃ³ricos

### 1. Coletar MÃ©tricas Reais (Ãšltimos 7 dias)

```powershell
# CPU Usage - HistÃ³rico P95
$cpuQuery = 'avg(rate(process_cpu_user_seconds_total[5m])) * 100'
$result = Invoke-RestMethod "http://localhost:9090/api/v1/query_range?query=$cpuQuery&start=$(Get-Date).AddDays(-7).ToUnixTimeSeconds()&end=$(Get-Date).ToUnixTimeSeconds()&step=3600"

# Calcular P95 dos Ãºltimos 7 dias
# Usar valor como baseline para threshold

# Memory Usage - HistÃ³rico P95
$memQuery = 'avg(process_resident_memory_bytes / process_virtual_memory_max_bytes) * 100'

# Latency P95 - HistÃ³rico
$latencyQuery = 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) * 1000'

# Error Rate - HistÃ³rico
$errorQuery = 'sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100'
```

### 2. Identificar PadrÃµes de Uso

**GrÃ¡ficos essenciais no Grafana**:
- CPU/Memory ao longo do tempo (7d)
- LatÃªncia por hora do dia (heatmap)
- Taxa de erro por dia da semana
- DistribuiÃ§Ã£o de trÃ¡fego (requests/min)

**Perguntas chave**:
- HÃ¡ picos previsÃ­veis? (ex: todo dia Ã s 9h)
- Fins de semana tÃªm perfil diferente?
- HÃ¡ manutenÃ§Ãµes recorrentes? (ex: backup noturno)
- UsuÃ¡rios de quais regiÃµes/timezones?

---

## ğŸ”§ Thresholds por MÃ©trica

### CPU Usage

**Threshold padrÃ£o**: 80% por 5 minutos

**Como calibrar**:

```promql
# No Grafana, executar query (Ãºltimos 7 dias):
histogram_quantile(0.95, 
  rate(process_cpu_seconds_total[5m])
) * 100

# Resultado exemplo: P95 = 62%
# Novo threshold: P95 + 20% = 62 * 1.2 = 74.4% â‰ˆ 75%
```

**Atualizar alert-rules.yml**:
```yaml
- alert: HighCPUUsage
  expr: avg(rate(process_cpu_seconds_total[5m])) * 100 > 75  # Era 80
  for: 5m
```

**RecomendaÃ§Ãµes**:
- ğŸŸ¢ Desenvolvimento: P95 + 30% (mais tolerante)
- ğŸŸ¡ Staging: P95 + 20%
- ğŸ”´ ProduÃ§Ã£o: P95 + 15% (mais sensÃ­vel)

### Memory Usage

**Threshold padrÃ£o**: 85% por 5 minutos

**Como calibrar**:

```promql
# Memory usage P95 (Ãºltimos 7 dias)
histogram_quantile(0.95,
  avg(process_resident_memory_bytes / process_virtual_memory_max_bytes) * 100
)

# Se P95 = 68%, threshold ideal = 68 * 1.2 = 81.6% â‰ˆ 82%
```

**Atualizar**:
```yaml
- alert: HighMemoryUsage
  expr: avg(process_resident_memory_bytes / process_virtual_memory_max_bytes) * 100 > 82
  for: 5m
```

**AtenÃ§Ã£o**: Memory leaks crescem gradualmente! Considerar:
- Adicionar alerta de **tendÃªncia** (crescimento >5% por hora)
- Alert de "Memory crescendo rÃ¡pido" separado

### Latency P95

**Threshold padrÃ£o**: 2000ms (2s)

**Como calibrar**:

```promql
# P95 latency (Ãºltimos 7 dias)
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket[5m])
) * 1000

# Se P95 = 850ms, threshold = 850 * 1.5 = 1275ms â‰ˆ 1300ms
```

**Latency por endpoint**:
```promql
# Alguns endpoints sÃ£o naturalmente mais lentos (ex: relatÃ³rios)
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket{endpoint="/api/reports"}[5m])
) * 1000

# Criar alert especÃ­fico se necessÃ¡rio
```

**Atualizar**:
```yaml
- alert: HighLatencyP95
  expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) * 1000 > 1300
  for: 3m
```

### Latency P99

**Threshold padrÃ£o**: 5000ms (5s)

**Como calibrar**:

```promql
# P99 latency
histogram_quantile(0.99, 
  rate(http_request_duration_seconds_bucket[5m])
) * 1000

# Se P99 = 2100ms, threshold = 2100 * 1.5 = 3150ms â‰ˆ 3200ms
```

**P99 Ã© volÃ¡til!** Considerar:
- DuraÃ§Ã£o (`for: 5m`) maior para evitar flapping
- Usar `avg_over_time` para suavizar picos

```yaml
- alert: HighLatencyP99
  expr: avg_over_time(histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) * 1000[10m]) > 3200
  for: 5m  # Mais tolerante
```

### Error Rate

**Threshold padrÃ£o**: 5% de erros 5xx

**Como calibrar**:

```promql
# Error rate mÃ©dio (Ãºltimos 7 dias)
sum(rate(http_requests_total{status=~"5.."}[5m])) 
/ 
sum(rate(http_requests_total[5m])) * 100

# Se mÃ©dia = 0.8%, threshold = 0.8 * 3 = 2.4% â‰ˆ 2.5%
```

**AtenÃ§Ã£o**: Error rate pode ser **zero** em apps saudÃ¡veis!
- Se P95 < 1%: threshold = 2-3%
- Se P95 = 1-5%: threshold = 5-8%
- Se P95 > 5%: **problema sistÃªmico, nÃ£o Ã© threshold!**

```yaml
- alert: HighHTTPErrorRate
  expr: sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100 > 2.5
  for: 2m
```

### Database Queries

**Threshold padrÃ£o**: 1000ms (1s)

**Como calibrar**:

```promql
# Query duration P95
histogram_quantile(0.95, 
  rate(database_query_duration_seconds_bucket[5m])
) * 1000

# Se P95 = 320ms, threshold = 320 * 2 = 640ms
```

**Por tipo de query**:
```promql
# SELECT vs INSERT vs UPDATE tÃªm perfis diferentes
histogram_quantile(0.95, 
  rate(database_query_duration_seconds_bucket{operation="SELECT"}[5m])
) * 1000
```

```yaml
- alert: SlowDatabaseQueries
  expr: histogram_quantile(0.95, rate(database_query_duration_seconds_bucket[5m])) * 1000 > 640
  for: 5m
```

### Disk Space

**Threshold padrÃ£o**: 90% usage

**Como calibrar**:

```promql
# Disk usage atual
(node_filesystem_size_bytes - node_filesystem_free_bytes) 
/ 
node_filesystem_size_bytes * 100

# Taxa de crescimento (GB por dia)
rate(node_filesystem_size_bytes - node_filesystem_free_bytes[24h]) / 1024^3
```

**EstratÃ©gia**:
- Alert 1: **Warning** aos 85% (tempo para reagir)
- Alert 2: **Critical** aos 95% (urgente!)
- Considerar: **Dias atÃ© cheio** baseado em taxa de crescimento

```yaml
- alert: DiskSpaceRunningOut
  expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 > 85
  for: 5m
  labels:
    severity: warning

- alert: DiskSpaceCritical
  expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 > 95
  for: 1m
  labels:
    severity: critical
```

---

## ğŸ“ˆ CalibraÃ§Ã£o por Ambiente

### Development

**Filosofia**: Tolerante, foco em detecÃ§Ã£o de problemas graves

```yaml
CPU: > 90% por 10min
Memory: > 90% por 10min
Latency P95: > 5000ms
Error Rate: > 10%
```

### Staging

**Filosofia**: Equilibrado, simular produÃ§Ã£o mas com mais margem

```yaml
CPU: > 80% por 5min
Memory: > 85% por 5min
Latency P95: > 2000ms
Error Rate: > 5%
```

### Production

**Filosofia**: SensÃ­vel, detecÃ§Ã£o precoce mas evitar falsos positivos

```yaml
CPU: > 75% por 3min
Memory: > 80% por 3min
Latency P95: > 1500ms
Error Rate: > 2%
```

---

## ğŸ§ª Testar Thresholds Ajustados

### MÃ©todo: Shadow Alerting

**Ideia**: Criar versÃ£o "shadow" do alerta com novo threshold, marcar como `severity: info` e monitorar por 1 semana.

```yaml
# Alert original (produÃ§Ã£o)
- alert: HighCPUUsage
  expr: avg(rate(process_cpu_seconds_total[5m])) * 100 > 80
  for: 5m
  labels:
    severity: warning

# Alert shadow (teste)
- alert: HighCPUUsage_Shadow
  expr: avg(rate(process_cpu_seconds_total[5m])) * 100 > 75
  for: 5m
  labels:
    severity: info
    shadow: "true"
  annotations:
    summary: "ğŸ§ª Shadow alert: testando threshold 75% (era 80%)"
```

**AnÃ¡lise apÃ³s 1 semana**:
- Quantos alertas shadow dispararam?
- Quantos foram legÃ­timos (problema real)?
- Taxa de falsos positivos < 10%? â†’ Threshold OK
- Taxa de falsos positivos > 20%? â†’ Threshold agressivo demais

---

## ğŸ“Š Dashboard de CalibraÃ§Ã£o

Criar dashboard no Grafana para monitorar efetividade:

**Painel 1: Alerts Disparados vs Problemas Reais**
```
(Firing Alerts / Total Alerts) * 100
```

**Painel 2: Falsos Positivos**
```
count(ALERTS{severity="warning"}) - count(incidents_confirmed)
```

**Painel 3: Threshold vs MÃ©trica Real**
- Linha 1: CPU threshold (80%)
- Linha 2: CPU P95 real (62%)
- Ãrea entre linhas = Margem de seguranÃ§a

**Painel 4: DistribuiÃ§Ã£o de Valores**
- Histograma mostrando onde mÃ©tricas ficam
- Threshold como linha vertical
- Idealmente: >95% dos valores abaixo do threshold

---

## ğŸ¯ CritÃ©rios de Sucesso

### Threshold Bem Calibrado

âœ… **Taxa de falsos positivos < 10%**  
âœ… **Tempo de detecÃ§Ã£o < 2min (MTTD)**  
âœ… **Zero falsos negativos** (problema real nÃ£o detectado)  
âœ… **Respeita variaÃ§Ã£o sazonal** (nÃ£o alerta em picos esperados)  
âœ… **Margem de seguranÃ§a de 15-20%** acima do P95 normal  

### Sinais de Threshold Ruim

âŒ **Falsos positivos > 30%** â†’ Muito sensÃ­vel  
âŒ **MTTD > 5min** â†’ Pouco sensÃ­vel  
âŒ **Alertas todo dia no mesmo horÃ¡rio** â†’ Ignorando padrÃ£o sazonal  
âŒ **Equipe ignorando alertas** â†’ Fadiga de alerta  

---

## ğŸ”„ Processo de Melhoria ContÃ­nua

### RevisÃ£o Semanal (15min)

1. Quantos alertas dispararam esta semana?
2. Quantos foram falsos positivos?
3. Houve problemas nÃ£o detectados?
4. Algum threshold precisa ajuste?

### RevisÃ£o Mensal (1h)

1. Analisar tendÃªncias (mÃ©tricas subindo/descendo?)
2. Recalcular P95/P99 com dados de 30 dias
3. Ajustar thresholds se necessÃ¡rio
4. Atualizar runbooks com novos aprendizados
5. Revisar SLO targets

### PÃ³s-Incidente

ApÃ³s cada incidente, perguntar:
- **Alert disparou a tempo?** Se nÃ£o â†’ Threshold menos sensÃ­vel
- **Houve falso alarme?** Se sim â†’ Threshold mais tolerante
- **Causa raiz coberta por alert?** Se nÃ£o â†’ Criar novo alert

---

## ğŸ“ Template de Ajuste

```yaml
# observability/threshold-adjustments.md

## [Data] - Ajuste de Threshold

### MÃ©trica: [Nome]
**Threshold anterior**: [Valor]  
**Threshold novo**: [Valor]  
**Motivo**: [Falsos positivos / DetecÃ§Ã£o tardia / PadrÃ£o sazonal]

### AnÃ¡lise
- P50 Ãºltimos 30d: [Valor]
- P95 Ãºltimos 30d: [Valor]
- P99 Ãºltimos 30d: [Valor]
- Falsos positivos anterior: [N] alertas/semana
- Problemas nÃ£o detectados: [N] incidentes

### Expectativa
- Reduzir falsos positivos em [X]%
- Manter MTTD < 2min
- Revisar em [Data + 1 semana]

### Resultado (preencher apÃ³s 1 semana)
- [ ] Falsos positivos reduziram?
- [ ] MTTD mantido?
- [ ] Sem falsos negativos?
- [ ] DecisÃ£o: Manter / Ajustar novamente / Reverter
```

---

## ğŸš€ Quick Wins

**Ajustes rÃ¡pidos que geram grande impacto**:

1. **Adicionar `for: 5m`** em todos os alerts â†’ Reduz flapping
2. **Usar `avg_over_time[10m]`** em mÃ©tricas volÃ¡teis â†’ Suaviza picos
3. **Criar alertas por horÃ¡rio** (business hours vs off-hours) â†’ Respeita sazonalidade
4. **Agrupar alertas relacionados** â†’ Evita spam (ex: APIDown inibe HighLatency)
5. **Silenciar manutenÃ§Ãµes programadas** â†’ Zero falsos positivos durante deploy

---

**Thresholds calibrados = Alertas confiÃ¡veis = Equipe feliz** ğŸ¯

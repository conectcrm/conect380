# ğŸ“Ÿ Guia de On-Call - ConectCRM

**Ãšltima atualizaÃ§Ã£o**: 2025-11-17  
**VersÃ£o**: 1.0  

---

## ğŸ¯ VisÃ£o Geral

Este guia orienta o plantonista on-call na resposta a incidentes do ConectCRM. Use-o durante seu turno para garantir resposta rÃ¡pida e estruturada.

---

## ğŸ“± InformaÃ§Ãµes de Contato

### EscalaÃ§Ã£o de Incidentes

**NÃ­vel 1 - Plantonista On-Call**  
- Resposta inicial: < 5 minutos  
- ResoluÃ§Ã£o esperada: < 15 minutos (crÃ­ticos)  
- Escala para L2 se: nÃ£o resolver em 15min OU necessitar acesso especial

**NÃ­vel 2 - Tech Lead / SRE**  
- Resposta: < 10 minutos apÃ³s escalaÃ§Ã£o  
- ResoluÃ§Ã£o esperada: < 30 minutos  
- Escala para L3 se: impacto sistÃªmico OU incidente prolongado

**NÃ­vel 3 - CTO / Arquiteto**  
- Resposta: < 15 minutos apÃ³s escalaÃ§Ã£o  
- DecisÃµes arquiteturais e mudanÃ§as emergenciais

### Canais de NotificaÃ§Ã£o

- ğŸ”´ **CrÃ­tico**: Slack #alerts-critical + SMS/Telefone
- ğŸŸ  **Warning**: Slack #alerts-warning  
- ğŸ”µ **Info**: Slack #alerts-info
- ğŸ“Š **SLO**: Slack #alerts-slo

---

## ğŸš¨ Ao Receber um Alerta

### Checklist Imediato (primeiros 2 minutos)

1. **Acknowledge o alerta** no Alertmanager
   - Acesse: http://localhost:9093
   - Clique no alerta â†’ Silence por 15min (tempo de investigaÃ§Ã£o)

2. **Avalie a severidade**
   - ğŸ”´ **Critical**: AÃ§Ã£o imediata, pode impactar clientes
   - ğŸŸ  **Warning**: Investigar, pode escalar para crÃ­tico
   - ğŸ”µ **Info**: Monitorar, nÃ£o requer aÃ§Ã£o urgente

3. **Verifique o contexto**
   - Dashboard Grafana: http://localhost:3002
   - Prometheus Alerts: http://localhost:9090/alerts
   - Logs no Loki: http://localhost:3002 (Explore â†’ Loki)

4. **Consulte o Runbook**
   - Abra: `observability/RUNBOOKS.md`
   - Localize o alerta especÃ­fico
   - Siga os passos de diagnÃ³stico

---

## ğŸ“‹ Procedimentos por Severidade

### ğŸ”´ Alertas CRÃTICOS (Critical)

**SLA**: Resposta < 5min | ResoluÃ§Ã£o < 15min

**AÃ§Ãµes ObrigatÃ³rias**:
1. âœ… Acknowledge imediato no Alertmanager
2. âœ… Silenciar por 15min (tempo de investigaÃ§Ã£o)
3. âœ… Notificar em #incidents: "ğŸš¨ Investigando [AlertName]"
4. âœ… Abrir dashboard de contexto (ver seÃ§Ã£o Dashboards)
5. âœ… Seguir runbook especÃ­fico
6. âœ… Escalar se nÃ£o resolver em 15min

**Alertas CrÃ­ticos**:
- `APIDown`: Backend nÃ£o responde â†’ Impacto total
- `DatabaseConnectionPoolExhausted`: DB sem conexÃµes â†’ Erros massivos
- `HighLatencyP99`: 1% usuÃ¡rios com 5s+ â†’ ExperiÃªncia ruim
- `DiskSpaceRunningOut`: Disco >90% â†’ Sistema pode travar
- `SLOAvailabilityViolation`: SLO quebrado â†’ Error budget esgotado

### ğŸŸ  Alertas WARNING (Warning)

**SLA**: Resposta < 15min | InvestigaÃ§Ã£o < 30min

**AÃ§Ãµes**:
1. âœ… Acknowledge no Alertmanager
2. âœ… Avaliar tendÃªncia (estÃ¡ piorando?)
3. âœ… Documentar observaÃ§Ãµes em thread do Slack
4. âœ… Seguir runbook para investigaÃ§Ã£o
5. âœ… Criar ticket se necessÃ¡rio follow-up
6. âœ… Escalar se tendÃªncia de piora continuar

**Alertas Warning**:
- `HighHTTPErrorRate`: >5% erros 5xx
- `HighLatencyP95`: 5% usuÃ¡rios com 2s+
- `SlowDatabaseQueries`: Queries >1s frequentes
- `HighCPUUsage`: CPU >80% por 5min
- `HighMemoryUsage`: Memory >85% por 5min

### ğŸ”µ Alertas INFO (Info)

**SLA**: Revisar no prÃ³ximo dia Ãºtil

**AÃ§Ãµes**:
1. âœ… Revisar contexto quando possÃ­vel
2. âœ… Documentar se padrÃ£o anormal
3. âœ… Criar ticket para otimizaÃ§Ã£o futura

---

## ğŸ”§ Ferramentas de DiagnÃ³stico

### Dashboards Essenciais

**Grafana** (http://localhost:3002):
- **Overview**: `/d/conectcrm-overview` - VisÃ£o geral do sistema
- **Alerting**: `/d/alerting-dashboard` - Status de alertas
- **Error Budget**: `/d/error-budget` - SLO e budget
- **Traces**: `/d/traces-dashboard` - Distributed tracing
- **Logs**: Explore â†’ Loki datasource

**Prometheus** (http://localhost:9090):
- **Alerts**: `/alerts` - Estado atual dos alertas
- **Targets**: `/targets` - SaÃºde dos endpoints
- **Graph**: `/graph` - Queries customizadas

**Alertmanager** (http://localhost:9093):
- **Alerts**: `/` - Gerenciar alertas ativos
- **Silences**: `/silences` - Gerenciar silenciamentos

**Jaeger** (http://localhost:16686):
- **Search**: Rastrear requisiÃ§Ãµes especÃ­ficas
- **Compare**: Comparar traces lentas vs rÃ¡pidas

### Comandos PowerShell Ãšteis

```powershell
# Verificar containers Docker
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# Logs de um container
docker logs -f --tail 100 conectsuite-backend

# SaÃºde do backend
Invoke-RestMethod http://localhost:3001/health

# Alertas ativos no Prometheus
Invoke-RestMethod http://localhost:9090/api/v1/alerts | ConvertTo-Json -Depth 10

# Alertas ativos no Alertmanager
Invoke-RestMethod http://localhost:9093/api/v2/alerts | ConvertTo-Json -Depth 10

# Reiniciar um serviÃ§o
docker-compose restart backend

# Ver uso de recursos
docker stats --no-stream

# Verificar portas em uso
netstat -ano | findstr "3001 9090 9093"
```

---

## ğŸ”‡ Como Silenciar Alertas

### Via Interface Web (Recomendado)

1. Acesse Alertmanager: http://localhost:9093
2. Localize o alerta ativo
3. Clique em **"Silence"**
4. Preencha:
   - **Duration**: 15min (investigaÃ§Ã£o), 1h (manutenÃ§Ã£o), 24h (conhecido)
   - **Creator**: Seu nome
   - **Comment**: Motivo do silenciamento (obrigatÃ³rio!)
5. Clique **"Create"**

### Via CLI (amtool)

```powershell
# Silenciar APIDown por 1 hora
docker exec conectsuite-alertmanager amtool silence add alertname=APIDown --duration=1h --comment="ManutenÃ§Ã£o programada" --author="Nome"

# Listar silenciamentos ativos
docker exec conectsuite-alertmanager amtool silence query

# Remover silenciamento
docker exec conectsuite-alertmanager amtool silence expire <silence-id>
```

### Boas PrÃ¡ticas de Silenciamento

âœ… **SEMPRE adicione comentÃ¡rio explicativo**  
âœ… Use duraÃ§Ã£o apropriada (15min-1h para investigaÃ§Ã£o)  
âœ… Documente em #incidents o motivo  
âœ… Remova silenciamento apÃ³s resolver  

âŒ **NUNCA silencie sem comentÃ¡rio**  
âŒ Nunca silencie por mais de 24h  
âŒ Nunca silencie alertas crÃ­ticos sem investigar  

---

## ğŸ“Š EscalaÃ§Ã£o de Incidentes

### Quando Escalar para L2 (Tech Lead/SRE)

- â±ï¸ NÃ£o resolveu em 15 minutos
- ğŸ” Precisa de acesso privilegiado (prod DB, secrets)
- ğŸ—ï¸ Requer mudanÃ§a arquitetural
- ğŸ“ˆ Impacto crescente (mais clientes afetados)
- ğŸ¤” Causa raiz nÃ£o identificada

### Quando Escalar para L3 (CTO/Arquiteto)

- ğŸ”¥ Incidente prolongado (>1h)
- ğŸ’¥ MÃºltiplos sistemas afetados
- ğŸ’° Impacto financeiro significativo
- ğŸ—£ï¸ ComunicaÃ§Ã£o externa necessÃ¡ria
- ğŸš¨ DecisÃ£o de negÃ³cio crÃ­tica (ex: rollback de release)

### Template de EscalaÃ§Ã£o

```
ğŸš¨ ESCALAÃ‡ÃƒO PARA L2/L3

Alerta: [Nome do Alerta]
Severidade: [Critical/Warning]
InÃ­cio: [HH:MM]
DuraÃ§Ã£o: [X minutos]

Sintomas:
- [O que estÃ¡ acontecendo]

JÃ¡ Tentado:
- [AÃ§Ãµes realizadas]

Impacto:
- [Quantos clientes afetados]
- [Funcionalidades comprometidas]

Contexto:
- Dashboard: [Link Grafana]
- Logs: [Link Loki com query]
- Traces: [Link Jaeger se aplicÃ¡vel]
```

---

## ğŸ“ Post-Incident Review

### ApÃ³s Resolver o Incidente

**Imediatamente**:
1. âœ… Remover silenciamentos
2. âœ… Confirmar mÃ©tricas normalizadas
3. âœ… Atualizar thread em #incidents com resoluÃ§Ã£o
4. âœ… Documentar aÃ§Ãµes tomadas

**AtÃ© 2h depois**:
1. âœ… Preencher Post-Mortem (use template em RUNBOOKS.md)
2. âœ… Identificar causa raiz
3. âœ… Propor aÃ§Ãµes corretivas
4. âœ… Criar tickets de follow-up

**AtÃ© 24h depois**:
1. âœ… Revisar Post-Mortem com equipe
2. âœ… Atualizar runbook se necessÃ¡rio
3. âœ… Ajustar thresholds se falso positivo

---

## ğŸ”„ Handoff de Turno

### Checklist ao Assumir Turno

```markdown
## ğŸŸ¢ Assumindo Turno - [Data] [HH:MM]

- [ ] Revisei alertas ativos no Alertmanager
- [ ] Chequei silenciamentos ativos e motivos
- [ ] Li threads em #incidents das Ãºltimas 24h
- [ ] Verifiquei dashboards estÃ£o carregando
- [ ] Testei acesso a ferramentas (Grafana, Prometheus, Alertmanager)
- [ ] Li handoff notes do turno anterior
- [ ] Confirmei contatos de escalaÃ§Ã£o disponÃ­veis

**Alertas Ativos**: [NÃºmero] - [Listar se houver]
**Silenciamentos**: [NÃºmero] - [Motivos]
**Incidentes Abertos**: [NÃºmero] - [Status]
**ObservaÃ§Ãµes**: [Qualquer contexto importante]

Plantonista: [Nome]
```

### Checklist ao Passar Turno

```markdown
## ğŸ”´ Passando Turno - [Data] [HH:MM]

**Resumo do Turno**:
- DuraÃ§Ã£o: [X horas]
- Alertas recebidos: [NÃºmero]
- Incidentes: [NÃºmero]

**Incidentes Tratados**:
1. [AlertName] - [HH:MM] - Resolvido/Escalado - [Breve descriÃ§Ã£o]

**PendÃªncias para PrÃ³ximo Turno**:
- [ ] [AÃ§Ã£o pendente 1]
- [ ] [AÃ§Ã£o pendente 2]

**Silenciamentos Ativos**:
- [AlertName] - Expira em [HH:MM] - Motivo: [X]

**ObservaÃ§Ãµes**:
- [Comportamentos anormais notados]
- [Trends preocupantes]
- [ManutenÃ§Ãµes programadas]

Plantonista saindo: [Nome]
Plantonista entrando: [Nome]
```

---

## ğŸ“ Dicas de Veterano

### Performance Under Pressure

1. **Respire**: 30 segundos para organizar pensamento valem mais que agir Ã s cegas
2. **Siga o Runbook**: Ele existe para te guiar sob pressÃ£o
3. **Documente enquanto investiga**: VocÃª vai esquecer detalhes depois
4. **PeÃ§a ajuda cedo**: Escalar nÃ£o Ã© falha, Ã© inteligÃªncia
5. **Comunique proativamente**: SilÃªncio gera ansiedade

### Comandos Salvadores

```powershell
# Ver Ãºltimos 50 logs de erro do backend
docker logs conectsuite-backend --tail 50 | Select-String "ERROR|FATAL"

# Identificar queries lentas no Postgres
docker exec -it conectsuite-postgres psql -U postgres -d conectcrm -c "SELECT pid, now() - query_start as duration, query FROM pg_stat_activity WHERE state = 'active' AND now() - query_start > interval '5 seconds' ORDER BY duration DESC;"

# Verificar uso de memÃ³ria de containers
docker stats --no-stream --format "table {{.Name}}\t{{.MemUsage}}\t{{.MemPerc}}"

# Tail logs de mÃºltiplos containers
docker-compose logs -f --tail=50 backend postgres redis

# Reiniciar stack completo (Ãºltimo recurso!)
docker-compose restart
```

### Red Flags ğŸš©

Estes sinais indicam problemas sÃ©rios - investigue imediatamente:

- ğŸš© CPU >95% por mais de 1 minuto
- ğŸš© MemÃ³ria >95% por mais de 30 segundos
- ğŸš© Disco >95% 
- ğŸš© Database connections >90% do pool
- ğŸš© LatÃªncia P99 >10s
- ğŸš© Taxa de erro >10%
- ğŸš© MÃºltiplos alertas crÃ­ticos simultÃ¢neos
- ğŸš© Logs mostrando "Out of Memory" ou "Connection refused"

---

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o TÃ©cnica

- **Runbooks**: `observability/RUNBOOKS.md`
- **Week 9 Doc**: `observability/WEEK_9_ALERTING_ONCALL.md`
- **Architecture**: `ANALISE_ARQUITETURA_OMNICHANNEL_COMPLETA.md`

### Dashboards de ReferÃªncia

- Overview System: http://localhost:3002/d/conectcrm-overview
- Error Budget: http://localhost:3002/d/error-budget
- Alerting: http://localhost:3002/d/alerting-dashboard

### Contatos de EmergÃªncia

```
Tech Lead:    [Nome] - [Telefone] - [Email]
SRE:          [Nome] - [Telefone] - [Email]
CTO:          [Nome] - [Telefone] - [Email]
DevOps:       [Nome] - [Telefone] - [Email]
```

---

## âœ… Quick Reference Card

**Imprima e mantenha perto durante turno on-call**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸš¨ QUICK REFERENCE - ON-CALL CONECTCRM           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“± ESCALAÃ‡ÃƒO:
   L1 â†’ VocÃª (< 5min response, < 15min resolve)
   L2 â†’ Tech Lead (< 10min response)
   L3 â†’ CTO (< 15min response)

ğŸ”— DASHBOARDS:
   Grafana:      http://localhost:3002
   Prometheus:   http://localhost:9090/alerts
   Alertmanager: http://localhost:9093
   Jaeger:       http://localhost:16686

ğŸ”§ COMANDOS RÃPIDOS:
   Logs:      docker logs -f --tail 100 conectsuite-backend
   Restart:   docker-compose restart backend
   Health:    Invoke-RestMethod http://localhost:3001/health
   Stats:     docker stats --no-stream

ğŸš¨ ALERTA RECEBIDO - AÃ‡ÃƒO IMEDIATA:
   1. Acknowledge no Alertmanager
   2. Silence 15min
   3. Abrir dashboard contexto
   4. Seguir runbook
   5. Documentar em #incidents
   6. Escalar se nÃ£o resolver em 15min

ğŸ“ COMUNICAÃ‡ÃƒO:
   Slack #incidents: AtualizaÃ§Ãµes de status
   Slack #alerts-*:  NotificaÃ§Ãµes automÃ¡ticas
   
ğŸ”‡ SILENCIAR ALERTA:
   http://localhost:9093 â†’ Alerta â†’ Silence
   SEMPRE adicionar comentÃ¡rio!

ğŸ“‹ HANDOFF:
   Revisar alertas ativos + silenciamentos
   Documentar pendÃªncias
   Passar contexto para prÃ³ximo turno

ğŸ†˜ EMERGÃŠNCIA:
   Tech Lead: [Telefone]
   Runbooks:  observability/RUNBOOKS.md
```

---

**Boa sorte no seu turno! ğŸš€**  
Lembre-se: VocÃª tem ferramentas, runbooks e equipe. NÃ£o estÃ¡ sozinho(a)!

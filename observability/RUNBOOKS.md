# üìñ Runbooks - ConectCRM Alerting
## Guia de Resposta a Incidentes (Week 9)

**Vers√£o**: 1.0  
**√öltima atualiza√ß√£o**: 2025-11-18  
**Equipe respons√°vel**: SRE / DevOps

---

## üìã √çndice

1. [Alertas Cr√≠ticos](#alertas-cr√≠ticos)
   - [APIDown](#1-apidown)
   - [DatabaseConnectionPoolExhausted](#2-databaseconnectionpoolexhausted)
   - [HighLatencyP99](#3-highlatencyp99)
   - [DiskSpaceRunningOut](#4-diskspacerunningout)
   - [SLOAvailabilityViolation](#5-sloavailabilityviolation)

2. [Alertas de Warning](#alertas-de-warning)
   - [HighHTTPErrorRate](#6-highhttperrorrate)
   - [HighLatencyP95](#7-highlatencyp95)
   - [SlowDatabaseQueries](#8-slowdatabasequeries)
   - [HighCPUUsage](#9-highcpuusage)
   - [HighMemoryUsage](#10-highmemoryusage)

3. [Alertas Baseados em Logs](#alertas-baseados-em-logs)
   - [CriticalLogDetected](#11-criticallogdetected)
   - [DatabaseConnectionErrors](#12-databaseconnectionerrors)
   - [MultipleFailedLogins](#13-multiplefailedlogins)
   - [PaymentProcessingErrors](#14-paymentprocessingerrors)

4. [Procedimentos Gerais](#procedimentos-gerais)
   - [Como Silenciar um Alerta](#como-silenciar-um-alerta)
   - [Como Escalar um Incidente](#como-escalar-um-incidente)
   - [Post-Mortem Template](#post-mortem-template)

---

## üö® Alertas Cr√≠ticos

### 1. APIDown

**Severidade**: üî¥ CRITICAL  
**Descri√ß√£o**: A API do ConectCRM est√° fora do ar e n√£o responde a requisi√ß√µes.

#### üéØ Impacto
- ‚ùå Clientes n√£o conseguem acessar o sistema
- ‚ùå Tickets n√£o podem ser criados/respondidos
- ‚ùå Dashboards n√£o carregam dados
- üí∞ **Impacto financeiro**: Alto (perda de receita)

#### üîç Diagn√≥stico

1. **Verificar se o backend est√° rodando**:
   ```powershell
   docker ps --filter "name=backend"
   ```
   
   - Se n√£o aparecer: Backend est√° parado ‚Üí Prosseguir para **A√ß√£o 1**
   - Se aparecer "unhealthy": Backend rodando mas com problemas ‚Üí **A√ß√£o 2**

2. **Verificar logs recentes**:
   ```powershell
   docker logs conectsuite-backend --tail 100
   ```
   
   Procurar por:
   - `Error: listen EADDRINUSE` ‚Üí Porta 3001 j√° em uso
   - `ECONNREFUSED` ‚Üí Banco de dados n√£o acess√≠vel
   - `Out of memory` ‚Üí Falta de mem√≥ria
   - Stack traces de exceptions n√£o tratadas

3. **Verificar sa√∫de do PostgreSQL**:
   ```powershell
   docker ps --filter "name=postgres"
   docker logs conectsuite-postgres --tail 50
   ```

#### ‚úÖ A√ß√µes de Resolu√ß√£o

**A√ß√£o 1: Backend parado (restart)**
```powershell
# 1. Reiniciar container
docker-compose restart backend

# 2. Aguardar 10 segundos
Start-Sleep -Seconds 10

# 3. Verificar sa√∫de
curl http://localhost:3001/health

# 4. Verificar logs de inicializa√ß√£o
docker logs conectsuite-backend --tail 50
```

**A√ß√£o 2: Backend unhealthy (investigar e restart)**
```powershell
# 1. Coletar logs detalhados
docker logs conectsuite-backend --tail 200 > backend-incident-$(Get-Date -Format 'yyyyMMdd-HHmmss').log

# 2. Verificar conex√£o com banco
Test-NetConnection -ComputerName localhost -Port 5432

# 3. Se DB est√° OK, restart for√ßado
docker-compose stop backend
docker-compose up -d backend

# 4. Monitorar startup
docker logs conectsuite-backend -f
```

**A√ß√£o 3: Banco de dados inacess√≠vel**
```powershell
# 1. Verificar se PostgreSQL est√° rodando
docker ps --filter "name=postgres"

# 2. Se n√£o est√° rodando, iniciar
docker-compose start postgres

# 3. Aguardar DB ficar pronto (15s)
Start-Sleep -Seconds 15

# 4. Testar conex√£o
docker exec conectsuite-postgres pg_isready -U postgres

# 5. Reiniciar backend ap√≥s DB subir
docker-compose restart backend
```

#### ‚è±Ô∏è SLA de Resolu√ß√£o
- **Detec√ß√£o ‚Üí A√ß√£o**: < 2 minutos
- **A√ß√£o ‚Üí Resolu√ß√£o**: < 5 minutos
- **MTTR Target**: < 7 minutos

#### üìä M√©tricas de Sucesso
- [ ] API respondendo em `http://localhost:3001/health`
- [ ] Prometheus scraping com sucesso (0 erros em 2 minutos)
- [ ] Logs sem exce√ß√µes cr√≠ticas
- [ ] Frontend carregando normalmente

#### üìù Checklist P√≥s-Incidente
- [ ] Documentar causa raiz no post-mortem
- [ ] Atualizar runbook se necess√°rio
- [ ] Criar task para prevenir reincid√™ncia
- [ ] Notificar equipe e stakeholders

---

### 2. DatabaseConnectionPoolExhausted

**Severidade**: üî¥ CRITICAL  
**Descri√ß√£o**: Pool de conex√µes com PostgreSQL est√° 90%+ ocupado.

#### üéØ Impacto
- ‚ö†Ô∏è Requisi√ß√µes lentas (esperam conex√£o dispon√≠vel)
- ‚ùå Timeouts em queries
- üí• Poss√≠vel queda da API se atingir 100%

#### üîç Diagn√≥stico

1. **Verificar estado atual do pool**:
   ```bash
   # Prometheus query
   (typeorm_connection_pool_active / typeorm_connection_pool_max) * 100
   ```

2. **Identificar conex√µes ativas no PostgreSQL**:
   ```sql
   SELECT 
     datname, 
     state, 
     COUNT(*) as connections,
     MAX(now() - query_start) as max_duration
   FROM pg_stat_activity 
   WHERE datname = 'conectcrm'
   GROUP BY datname, state;
   ```

3. **Identificar queries lentas (> 5s)**:
   ```sql
   SELECT 
     pid, 
     now() - query_start AS duration, 
     state, 
     query 
   FROM pg_stat_activity 
   WHERE state != 'idle' 
     AND now() - query_start > interval '5 seconds'
   ORDER BY duration DESC;
   ```

#### ‚úÖ A√ß√µes de Resolu√ß√£o

**A√ß√£o 1: Aumentar pool temporariamente (curto prazo)**
```typescript
// backend/src/config/database.config.ts
extra: {
  max: 20,  // Aumentar de 10 para 20
  min: 2,
  idleTimeoutMillis: 30000,
}
```

**A√ß√£o 2: Matar queries lentas (emerg√™ncia)**
```sql
-- Identificar PIDs de queries > 30s
SELECT pg_terminate_backend(pid) 
FROM pg_stat_activity 
WHERE state != 'idle' 
  AND now() - query_start > interval '30 seconds'
  AND datname = 'conectcrm';
```

**A√ß√£o 3: Reiniciar backend (liberar conex√µes travadas)**
```powershell
docker-compose restart backend
```

#### üõ†Ô∏è Preven√ß√£o (Longo Prazo)
- [ ] Otimizar queries lentas (adicionar √≠ndices)
- [ ] Implementar connection pooling externo (PgBouncer)
- [ ] Monitorar query performance (APM)
- [ ] Configurar query timeout (30s)

---

### 3. HighLatencyP99

**Severidade**: üî¥ CRITICAL  
**Descri√ß√£o**: 1% das requisi√ß√µes com lat√™ncia > 5 segundos.

#### üéØ Impacto
- üò§ Usu√°rios experimentando lentid√£o extrema
- üìâ Degrada√ß√£o da experi√™ncia (UX ruim)
- ‚ö†Ô∏è SLO de lat√™ncia em risco

#### üîç Diagn√≥stico

1. **Identificar rotas afetadas**:
   ```promql
   topk(5, 
     histogram_quantile(0.99, 
       sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route)
     )
   )
   ```

2. **Verificar traces no Jaeger**:
   - Acessar: http://localhost:16686
   - Filtrar por: `duration > 5s`
   - Ordenar por: Longest First
   - Analisar spans lentos (DB, external APIs, processing)

3. **Verificar logs de slow queries**:
   ```
   {job="conectcrm-backend"} |~ "(?i)query.*timeout|slow.*query"
   ```

#### ‚úÖ A√ß√µes de Resolu√ß√£o

**A√ß√£o 1: Cache tempor√°rio em rotas lentas**
```typescript
// Adicionar cache in-memory (15 segundos)
import { CacheInterceptor } from '@nestjs/cache-manager';

@UseInterceptors(CacheInterceptor)
@CacheTTL(15)
@Get('/rota-lenta')
async rotaLenta() { ... }
```

**A√ß√£o 2: Otimizar query espec√≠fica**
- Identificar query no trace do Jaeger
- Analisar EXPLAIN ANALYZE no PostgreSQL
- Adicionar √≠ndice se necess√°rio

**A√ß√£o 3: Escalar recursos (curto prazo)**
- Aumentar CPU/RAM do backend (se containerizado)
- Ou reiniciar para limpar cache/mem√≥ria

---

### 4. DiskSpaceRunningOut

**Severidade**: üî¥ CRITICAL  
**Descri√ß√£o**: Disco com mais de 90% de uso.

#### üéØ Impacto
- ‚ùå Backend n√£o consegue escrever logs
- ‚ùå PostgreSQL n√£o consegue gravar dados
- üí• Sistema pode falhar completamente

#### üîç Diagn√≥stico

```powershell
# Verificar uso de disco
Get-PSDrive -PSProvider FileSystem

# Identificar maiores diret√≥rios
Get-ChildItem -Path C:\Projetos\conectcrm -Recurse -Directory |
  ForEach-Object {
    $size = (Get-ChildItem $_.FullName -Recurse -File | Measure-Object -Property Length -Sum).Sum / 1GB
    [PSCustomObject]@{
      Path = $_.FullName
      SizeGB = [Math]::Round($size, 2)
    }
  } | Sort-Object SizeGB -Descending | Select-Object -First 10
```

#### ‚úÖ A√ß√µes de Resolu√ß√£o

**A√ß√£o 1: Limpar logs antigos**
```powershell
# Remover logs > 7 dias
Get-ChildItem -Path C:\Projetos\conectcrm\backend\logs -Recurse -File |
  Where-Object { $_.LastWriteTime -lt (Get-Date).AddDays(-7) } |
  Remove-Item -Force

# Remover Docker images n√£o utilizadas
docker image prune -a --filter "until=168h" -f
```

**A√ß√£o 2: Limpar volumes Docker √≥rf√£os**
```powershell
docker volume prune -f
```

**A√ß√£o 3: Rotacionar logs do PostgreSQL**
```sql
-- Limpar logs antigos do PG
SELECT pg_rotate_logfile();
```

---

### 5. SLOAvailabilityViolation

**Severidade**: üî¥ CRITICAL  
**Descri√ß√£o**: Success rate caiu abaixo de 99.9% no per√≠odo mensal.

#### üéØ Impacto
- üìâ SLO violado (contrato com clientes)
- üí∏ Poss√≠vel cr√©dito/reembolso para clientes
- üìä Error budget esgotado

#### üîç Diagn√≥stico

1. **Verificar taxa de erros atual**:
   ```promql
   1 - (sum(rate(http_requests_total{status!~"5.."}[30d])) / sum(rate(http_requests_total[30d])))
   ```

2. **Identificar picos de erro**:
   - Dashboard: Week 6 - Error Budget Management
   - Gr√°fico: Error Rate Over Time

3. **Analisar logs de erros**:
   ```
   {job="conectcrm-backend", level="error"} 
   | json 
   | line_format "{{.timestamp}} [{{.context}}] {{.message}}"
   ```

#### ‚úÖ A√ß√µes de Resolu√ß√£o

**A√ß√£o 1: Comunicar stakeholders**
- Notificar ger√™ncia sobre viola√ß√£o de SLO
- Preparar relat√≥rio de impacto
- Estimar tempo de recupera√ß√£o

**A√ß√£o 2: Parar deploys (freeze)**
- Impedir novos deploys at√© estabilizar
- Rollback da √∫ltima vers√£o se necess√°rio

**A√ß√£o 3: Investigar causa raiz**
- Revisar deploys recentes
- Analisar mudan√ßas de configura√ß√£o
- Verificar depend√™ncias externas (APIs)

---

## ‚ö†Ô∏è Alertas de Warning

### 6. HighHTTPErrorRate

**Descri√ß√£o**: Taxa de erros HTTP 5xx > 5% por 3 minutos.

#### ‚úÖ A√ß√µes
1. Verificar logs: `{level="error"} |~ "5\\d\\d"`
2. Identificar rota espec√≠fica com erro
3. Checar traces no Jaeger
4. Se persistir > 10min, escalar para CRITICAL

---

### 7. HighLatencyP95

**Descri√ß√£o**: 5% das requisi√ß√µes com lat√™ncia > 2s.

#### ‚úÖ A√ß√µes
1. Identificar rota lenta no Grafana
2. Analisar trace no Jaeger
3. Verificar slow queries no PostgreSQL
4. Adicionar cache se necess√°rio

---

### 8. SlowDatabaseQueries

**Descri√ß√£o**: P95 de query time > 1 segundo.

#### ‚úÖ A√ß√µes
1. Identificar entity/tabela:
   ```promql
   topk(5, histogram_quantile(0.95, 
     sum(rate(typeorm_query_duration_seconds_bucket[5m])) by (le, entity)
   ))
   ```

2. Executar EXPLAIN ANALYZE:
   ```sql
   EXPLAIN (ANALYZE, BUFFERS) SELECT ...;
   ```

3. Criar √≠ndice se necess√°rio:
   ```sql
   CREATE INDEX idx_tickets_status ON tickets(status);
   ```

---

## üìù Alertas Baseados em Logs

### 11. CriticalLogDetected

**Descri√ß√£o**: Logs com n√≠vel CRITICAL registrados.

#### ‚úÖ A√ß√µes
1. Verificar log espec√≠fico:
   ```
   {job="conectcrm-backend", level="critical"} | json
   ```

2. Identificar contexto (context, trace_id)
3. Rastrear no Jaeger pelo trace_id
4. Corrigir c√≥digo se bug encontrado

---

### 12. DatabaseConnectionErrors

**Descri√ß√£o**: Erros de conex√£o com PostgreSQL nos logs.

#### ‚úÖ A√ß√µes
1. Verificar se PostgreSQL est√° rodando:
   ```powershell
   docker ps --filter "name=postgres"
   ```

2. Testar conectividade:
   ```powershell
   Test-NetConnection -ComputerName localhost -Port 5432
   ```

3. Verificar logs do PG:
   ```powershell
   docker logs conectsuite-postgres --tail 100
   ```

4. Reiniciar PostgreSQL se necess√°rio:
   ```powershell
   docker-compose restart postgres
   ```

---

### 13. MultipleFailedLogins

**Descri√ß√£o**: Mais de 20 tentativas de login falhadas em 5 minutos.

#### ‚úÖ A√ß√µes
1. Identificar IPs suspeitos:
   ```
   {job="conectcrm-backend"} |~ "(?i)failed.*login" 
   | json 
   | line_format "{{.ip}} {{.username}}"
   ```

2. Bloquear IP temporariamente (se ataque):
   ```typescript
   // Adicionar ao rate limiter
   @Throttle(5, 60) // 5 tentativas por minuto
   @Post('/login')
   async login() { ... }
   ```

3. Notificar seguran√ßa se padr√£o de ataque

---

### 14. PaymentProcessingErrors

**Descri√ß√£o**: Erros cr√≠ticos no fluxo de pagamentos.

#### ‚úÖ A√ß√µes
1. Verificar transa√ß√µes afetadas:
   ```
   {job="conectcrm-backend"} |~ "(?i)payment.*error" | json
   ```

2. Checar Stripe Dashboard:
   - https://dashboard.stripe.com/payments
   - Verificar failed payments

3. Notificar time financeiro imediatamente
4. Reprocessar pagamentos manualmente se necess√°rio

---

## üõ†Ô∏è Procedimentos Gerais

### Como Silenciar um Alerta

**Quando usar**: Manuten√ß√£o programada, falso positivo, incidente j√° conhecido.

**Via Alertmanager UI**:
1. Acessar: http://localhost:9093
2. Clicar no alerta
3. "Silence" ‚Üí Preencher:
   - Duration: 1h, 4h, 24h
   - Comment: "Manuten√ß√£o programada - deploy v2.1.0"
   - Created by: Seu nome
4. Confirmar

**Via CLI (amtool)**:
```powershell
docker exec conectsuite-alertmanager amtool silence add alertname="HighLatencyP95" --duration=2h --comment="Investigating"
```

---

### Como Escalar um Incidente

**N√≠veis de Escala√ß√£o**:

**N√≠vel 1 - SRE On-Call** (voc√™):
- Alerts WARNING e INFO
- Troubleshooting inicial

**N√≠vel 2 - SRE Lead**:
- Alerts CRITICAL n√£o resolvidos em 15min
- Impacto em m√∫ltiplos clientes
- SLO violation

**N√≠vel 3 - Engineering Manager + CTO**:
- Outage completo > 30min
- Perda de dados
- Incidente de seguran√ßa

**Canais de Comunica√ß√£o**:
- Slack: `#incidents`
- Email: `sre-oncall@conectcrm.com`
- Phone: (emerg√™ncia apenas)

---

### Post-Mortem Template

```markdown
# Post-Mortem: [T√≠tulo do Incidente]

**Data**: 2025-11-18  
**Severidade**: Critical  
**Dura√ß√£o**: 15 minutos (10:30 - 10:45 UTC)  
**Impacto**: 1.200 usu√°rios afetados, 500 requisi√ß√µes falhadas

## üìã Sum√°rio Executivo
[Descri√ß√£o em 2-3 par√°grafos sobre o que aconteceu]

## ‚è±Ô∏è Timeline
- 10:30 - Alert disparado: APIDown
- 10:32 - On-call inicia investiga√ß√£o
- 10:35 - Identificada causa raiz: PostgreSQL connection pool exausto
- 10:40 - A√ß√£o implementada: Pool aumentado de 10 para 20 conex√µes
- 10:43 - Backend reiniciado
- 10:45 - Sistema restaurado, alerta resolvido

## üîç Causa Raiz
[An√°lise detalhada t√©cnica do que causou o incidente]

## ‚úÖ A√ß√µes Tomadas
1. Aumentado pool de conex√µes PostgreSQL
2. Reiniciado backend
3. Monitorado recovery por 15 minutos

## üõ°Ô∏è Preven√ß√£o (Action Items)
- [ ] Implementar auto-scaling de pool de conex√µes
- [ ] Adicionar alerta preventivo em 80% de uso
- [ ] Documentar troubleshooting de pool exhaustion
- [ ] Review code: identificar queries que n√£o fecham conex√µes

## üìä M√©tricas
- MTTR: 15 minutos ‚úÖ (target < 30min)
- MTTD: 2 minutos ‚úÖ (alerting funcionou)
- Impact: 1.200 usu√°rios, 0,5% da base
```

---

## üîó Links √öteis

- **Prometheus Alerts**: http://localhost:9090/alerts
- **Alertmanager UI**: http://localhost:9093
- **Grafana Alerting Dashboard**: http://localhost:3002/d/alerting-dashboard
- **Jaeger Traces**: http://localhost:16686
- **Loki Logs**: http://localhost:3002/explore (datasource: Loki)

---

**Vers√£o do documento**: 1.0  
**Pr√≥xima revis√£o**: Ap√≥s cada incidente ou mensalmente  
**Mantenedores**: Equipe SRE ConectCRM

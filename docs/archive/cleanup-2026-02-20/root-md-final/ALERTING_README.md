# ğŸš¨ Sistema de Alerting & SLOs - ConectCRM

Sistema completo de alertas inteligentes, roteamento multi-canal e monitoramento de Service Level Objectives (SLOs).

---

## ğŸ“‹ Ãndice

1. [Quickstart](#-quickstart)
2. [Arquitetura](#-arquitetura)
3. [Alertas Configurados](#-alertas-configurados)
4. [SLOs Definidos](#-slos-definidos)
5. [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
6. [Testando Alertas](#-testando-alertas)
7. [Runbooks](#-runbooks)
8. [Troubleshooting](#-troubleshooting)

---

## ğŸš€ Quickstart

### 1. Subir a Stack de Observabilidade

```powershell
# Copiar arquivo de exemplo
Copy-Item .env.alerting.example .env.alerting

# Editar variÃ¡veis (SLACK_WEBHOOK_URL, SMTP_*, etc.)
notepad .env.alerting

# Iniciar serviÃ§os
docker-compose up -d prometheus alertmanager grafana

# Verificar status
docker-compose ps
```

### 2. Acessar Interfaces

| ServiÃ§o | URL | Credenciais |
|---------|-----|-------------|
| **Prometheus** | http://localhost:9090 | - |
| **Alertmanager** | http://localhost:9093 | - |
| **Grafana** | http://localhost:3002 | admin/admin |

### 3. Testar Sistema

```powershell
# Testar todos os alertas
.\scripts\test-alerting.ps1 -Severity all

# Testar apenas crÃ­ticos
.\scripts\test-alerting.ps1 -Severity critical

# Ver alertas ativos
Start-Process "http://localhost:9093/#/alerts"
```

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COLETA DE MÃ‰TRICAS                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Backend NestJS (localhost:3001/metrics)                   â”‚
â”‚     â†“                                                      â”‚
â”‚  Prometheus (scrape a cada 15s)                            â”‚
â”‚     â†“                                                      â”‚
â”‚  Avalia Alert Rules (a cada 30s)                           â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ROTEAMENTO DE ALERTAS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Alertmanager                                              â”‚
â”‚     â”œâ”€ Critical â†’ Email + Slack + PagerDuty (0s wait)      â”‚
â”‚     â”œâ”€ Warning  â†’ Email + Slack (30s wait)                 â”‚
â”‚     â”œâ”€ Info     â†’ Slack apenas (5m wait)                   â”‚
â”‚     â””â”€ SLO      â†’ #slo-violations (1h wait)                â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      VISUALIZAÃ‡ÃƒO                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Grafana Dashboards                                        â”‚
â”‚     â”œâ”€ SLO Overview                                        â”‚
â”‚     â”œâ”€ Error Budget                                        â”‚
â”‚     â”œâ”€ Alert History                                       â”‚
â”‚     â””â”€ Performance Metrics                                 â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”” Alertas Configurados

### Disponibilidade (3 alertas)

| Alerta | Severidade | CondiÃ§Ã£o | DuraÃ§Ã£o | Runbook |
|--------|------------|----------|---------|---------|
| **APIDown** | ğŸ”´ Critical | `up{job="nestjs-api"} == 0` | 1min | [api-down.md](../backend/docs/runbooks/api-down.md) |
| **DatabaseConnectionPoolExhausted** | ğŸ”´ Critical | Pool > 90% | 5min | [db-pool-exhausted.md](../backend/docs/runbooks/db-pool-exhausted.md) |
| **HighHTTPErrorRate** | ğŸŸ¡ Warning | > 5% erros 5xx | 3min | - |

### Performance (3 alertas)

| Alerta | Severidade | CondiÃ§Ã£o | DuraÃ§Ã£o | Runbook |
|--------|------------|----------|---------|---------|
| **HighLatencyP95** | ğŸŸ¡ Warning | P95 > 2s | 5min | - |
| **HighLatencyP99** | ğŸ”´ Critical | P99 > 5s | 3min | - |
| **SlowDatabaseQueries** | ğŸŸ¡ Warning | Query P95 > 1s | 5min | - |

### Recursos (3 alertas)

| Alerta | Severidade | CondiÃ§Ã£o | DuraÃ§Ã£o |
|--------|------------|----------|---------|
| **HighCPUUsage** | ğŸŸ¡ Warning | CPU > 80% | 10min |
| **HighMemoryUsage** | ğŸŸ¡ Warning | MemÃ³ria > 85% | 5min |
| **DiskSpaceRunningOut** | ğŸ”´ Critical | Disco > 90% | 5min |

### Atendimento (3 alertas)

| Alerta | Severidade | CondiÃ§Ã£o | DuraÃ§Ã£o |
|--------|------------|----------|---------|
| **HighTicketQueueSize** | ğŸŸ¡ Warning | > 50 tickets na fila | 10min |
| **SlowTicketResponseTime** | ğŸŸ¡ Warning | MÃ©dia > 30min | 15min |
| **HighTicketAbandonmentRate** | ğŸ”´ Critical | > 15% abandono | 30min |

### SLOs (1 alerta)

| Alerta | Severidade | CondiÃ§Ã£o | DuraÃ§Ã£o |
|--------|------------|----------|---------|
| **ErrorBudgetExhausted** | ğŸ”´ Critical | > 80% budget consumido | 1h |

### Business (2 alertas)

| Alerta | Severidade | CondiÃ§Ã£o | DuraÃ§Ã£o |
|--------|------------|----------|---------|
| **TrafficDropDetected** | ğŸŸ¡ Warning | Queda > 50% trÃ¡fego | 10min |
| **LowConversionRate** | ğŸ”µ Info | < 50% tickets resolvidos | 2h |

**Total: 14 alertas** (5 critical, 7 warning, 2 info)

---

## ğŸ“Š SLOs Definidos

### SLO 1: Disponibilidade
- **Target**: 99.9% (30 dias)
- **Error Budget**: 0.1% = **43 minutos/mÃªs**
- **SLI**: `(success_requests / total_requests) * 100`
- **Alerta**: SLOAvailabilityViolation

### SLO 2: LatÃªncia
- **Target**: P95 < 2s (7 dias)
- **Error Budget**: 5%
- **SLI**: `histogram_quantile(0.95, http_request_duration_seconds)`
- **Alerta**: SLOLatencyViolation

### SLO 3: Taxa de Erros
- **Target**: < 0.1% erros 5xx (30 dias)
- **Error Budget**: 0.1%
- **SLI**: `(5xx_requests / total_requests) * 100`
- **Alerta**: HighErrorRate

### SLO 4: Primeira Resposta
- **Target**: P90 < 30min (7 dias)
- **Error Budget**: 10%
- **SLI**: `histogram_quantile(0.90, ticket_tempo_primeira_resposta_seconds)`
- **Alerta**: SlowFirstResponse

### SLO 5: Tempo de ResoluÃ§Ã£o
- **Target**: P80 < 4h (30 dias)
- **Error Budget**: 20%
- **SLI**: `histogram_quantile(0.80, ticket_tempo_resolucao_seconds)`
- **Alerta**: SlowResolutionTime

### SLO 6: Database Latency
- **Target**: P95 < 500ms (1 dia)
- **Error Budget**: 5%
- **SLI**: `histogram_quantile(0.95, typeorm_query_duration_seconds)`
- **Alerta**: SlowDatabaseQueries

### SLO 7: Taxa de ConversÃ£o
- **Target**: > 60% tickets resolvidos (7 dias)
- **Error Budget**: 10%
- **SLI**: `(tickets_resolvidos / tickets_criados) * 100`
- **Alerta**: LowConversionRate

---

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. VariÃ¡veis de Ambiente

Criar arquivo `.env.alerting` a partir do `.env.alerting.example`:

```bash
# Slack
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# PagerDuty
PAGERDUTY_SERVICE_KEY=your-pagerduty-integration-key

# SMTP (Email)
SMTP_USERNAME=alerts@conectcrm.com
SMTP_PASSWORD=your-smtp-app-password

# Grafana
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin123
```

#### Alertas internos (fila `notifications` no backend)

No backend, alertas crÃ­ticos sÃ£o enfileirados via `notify-user` para o admin configurado:

- `NOTIFICATIONS_ADMIN_USER_ID`: **obrigatÃ³rio** para receber alertas internos (breaker, backlog alto e SLA em risco/violado). Sem esse valor, nenhum alerta interno Ã© enviado.
- `NOTIFICATIONS_BACKLOG_THRESHOLD`: opcional; se definido, alerta de backlog alto da fila de notificaÃ§Ãµes (cooldown 5min).

Detalhes de comportamento:
- SLA: tickets em risco/violados geram `notify-user` deduplicado por status/ticket e nÃ£o interrompem o fluxo de SLA se a fila falhar.
- Breaker/backlog: abertura de breaker ou backlog acima do limiar envia `notify-user` para o admin quando configurado.
- Email: handler real via SMTP (`SEND_EMAIL`) com retry/jitter; na Ãºltima tentativa falha notifica admin via `notify-user` com contexto.
- WhatsApp/SMS/Push: handlers ainda no-op; a cada job Ã© enviada notificaÃ§Ã£o ao admin informando que o canal estÃ¡ em modo no-op (payload inclui `context`, `jobId` e destinatÃ¡rio).

### 2. Configurar Slack

1. Criar app em: https://api.slack.com/apps
2. Ativar "Incoming Webhooks"
3. Criar canais:
   - `#alerts-critical`
   - `#alerts-warning`
   - `#alerts-info`
   - `#slo-violations`
4. Adicionar webhook para cada canal
5. Copiar URL do webhook

### 3. Configurar PagerDuty

1. Criar service em PagerDuty
2. Adicionar integraÃ§Ã£o "Events API v2"
3. Copiar "Integration Key"
4. Configurar escalaÃ§Ã£o:
   - NÃ­vel 1: On-call engineer (imediato)
   - NÃ­vel 2: Tech Lead (apÃ³s 5min)
   - NÃ­vel 3: CTO (apÃ³s 10min)

### 4. Configurar SMTP (Gmail)

1. Acessar: https://myaccount.google.com/apppasswords
2. Criar "App Password" para "Mail"
3. Copiar senha de 16 caracteres
4. Usar em `SMTP_PASSWORD`

---

## ğŸ§ª Testando Alertas

### Teste Completo (Todos os Alertas)

```powershell
.\scripts\test-alerting.ps1 -Severity all
```

### Teste por Severidade

```powershell
# Apenas crÃ­ticos
.\scripts\test-alerting.ps1 -Severity critical

# Apenas warnings
.\scripts\test-alerting.ps1 -Severity warning

# Apenas informativos
.\scripts\test-alerting.ps1 -Severity info

# Apenas SLOs
.\scripts\test-alerting.ps1 -Severity slo
```

### Teste Manual (cURL)

```powershell
# Enviar alerta de teste
$alert = @{
    labels = @{
        alertname = "APIDown"
        severity = "critical"
        instance = "localhost:3001"
    }
    annotations = @{
        summary = "Teste manual de alerta"
    }
} | ConvertTo-Json

Invoke-RestMethod `
    -Uri "http://localhost:9093/api/v1/alerts" `
    -Method Post `
    -Body "[$alert]" `
    -ContentType "application/json"
```

### Verificar Alertas Ativos

```powershell
# Via API
Invoke-RestMethod -Uri "http://localhost:9093/api/v1/alerts" | ConvertTo-Json

# Via UI
Start-Process "http://localhost:9093/#/alerts"
```

---

## ğŸ“š Runbooks

Runbooks operacionais para resoluÃ§Ã£o de incidentes:

### DisponÃ­veis
- âœ… [API Down](../backend/docs/runbooks/api-down.md) - RTO: 5min
- âœ… [DB Pool Exhausted](../backend/docs/runbooks/db-pool-exhausted.md) - AnÃ¡lise de root cause

### Em Desenvolvimento
- â³ High Latency - OtimizaÃ§Ã£o de performance
- â³ High Error Rate - AnÃ¡lise de erros 5xx
- â³ SLO Violation - Procedimentos gerais

---

## ğŸ”§ Troubleshooting

### Alertmanager nÃ£o inicia

```powershell
# Verificar logs
docker-compose logs alertmanager

# Verificar config
docker-compose exec alertmanager amtool check-config /etc/alertmanager/alertmanager.yml

# Restart
docker-compose restart alertmanager
```

### Alertas nÃ£o chegam no Slack

**Problema**: VariÃ¡vel `SLACK_WEBHOOK_URL` nÃ£o configurada

**SoluÃ§Ã£o**:
```powershell
# Verificar variÃ¡vel
docker-compose exec alertmanager env | Select-String SLACK

# Se vazio, adicionar em .env.alerting e restart
docker-compose restart alertmanager
```

### Prometheus nÃ£o envia alertas

**Problema**: Alertmanager nÃ£o estÃ¡ configurado no Prometheus

**SoluÃ§Ã£o**:
```yaml
# Verificar observability/prometheus.yml
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']
```

```powershell
# Reload config
Invoke-RestMethod -Uri "http://localhost:9090/-/reload" -Method Post

# Verificar status
Start-Process "http://localhost:9090/config"
```

### Alert Rules nÃ£o carregam

**Problema**: Arquivo nÃ£o montado corretamente

**SoluÃ§Ã£o**:
```powershell
# Verificar se arquivo existe
Test-Path "backend/config/alert-rules.yml"

# Verificar mount no container
docker-compose exec prometheus ls -la /etc/prometheus/

# Validar sintaxe
docker-compose exec prometheus promtool check rules /etc/prometheus/alert-rules.yml
```

### Grafana nÃ£o mostra datasource

**Problema**: Prometheus datasource nÃ£o provisionado

**SoluÃ§Ã£o**:
```powershell
# Verificar provisioning
docker-compose exec grafana ls -la /etc/grafana/provisioning/datasources/

# Restart Grafana
docker-compose restart grafana

# Verificar datasources via API
Invoke-RestMethod `
    -Uri "http://localhost:3002/api/datasources" `
    -Headers @{ Authorization = "Basic " + [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes("admin:admin")) }
```

---

## ğŸ“Š Dashboards Grafana

### SLO Overview
- Disponibilidade 30d, 7d, 1d
- LatÃªncia P50, P95, P99
- Error rate
- Ticket metrics (resposta, resoluÃ§Ã£o, conversÃ£o)

### Error Budget
- Budget restante por SLO
- Burn rate (taxa de consumo)
- Dias atÃ© esgotar
- HistÃ³rico de violaÃ§Ãµes

### Alert History
- Alertas disparados nas Ãºltimas 24h
- Tempo mÃ©dio de resoluÃ§Ã£o (MTTR)
- Alertas por severidade
- Top 10 alertas mais frequentes

---

## ğŸ¯ Error Budget Policy

| Budget Restante | Status | AÃ§Ã£o | Deploy Frequency |
|-----------------|--------|------|------------------|
| **> 80%** | ğŸŸ¢ Normal | OperaÃ§Ãµes normais | MÃºltiplos/dia |
| **50-80%** | ğŸŸ¡ Caution | Revisar mudanÃ§as | 1-2/dia |
| **20-50%** | ğŸŸ  Warning | Foco em confiabilidade | EmergÃªncias apenas |
| **< 20%** | ğŸ”´ **FREEZE** | **DEPLOY FREEZE** | Critical fixes only |

---

## ğŸ”— Links Ãšteis

- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Prometheus Alerting](https://prometheus.io/docs/alerting/latest/overview/)
- [Alertmanager Config](https://prometheus.io/docs/alerting/latest/configuration/)
- [SLO Workshop by Google](https://sre.google/workbook/slo-engineering/)
- [Error Budget Policy](https://sre.google/workbook/implementing-slos/#defending_slos)

---

**Status**: âœ… Sistema completo implementado!  
**Ãšltima atualizaÃ§Ã£o**: 17 de novembro de 2025

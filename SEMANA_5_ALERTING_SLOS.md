# Semana 5: Alerting & SLOs - ImplementaÃ§Ã£o Completa

## ğŸ¯ Objetivo
Implementar sistema completo de alertas inteligentes e SLOs (Service Level Objectives) para garantir disponibilidade e performance do ConectCRM.

---

## âœ… O Que Foi Implementado

### 1. **Alertmanager Configuration** (`config/alertmanager.yml`)

#### Roteamento Inteligente
- **4 nÃ­veis de severidade**: critical, warning, info, SLO
- **MÃºltiplos canais**: Email, Slack, PagerDuty
- **InibiÃ§Ã£o automÃ¡tica**: Warning inibido quando Critical ativo
- **Group by**: Agrupa alertas relacionados

#### Receivers Configurados
```yaml
critical-alerts:  â†’ Email + Slack + PagerDuty
warning-alerts:   â†’ Email + Slack
info-alerts:      â†’ Slack apenas
slo-alerts:       â†’ Canal dedicado #slo-violations
```

#### Timings Otimizados
- **group_wait**: 0s para crÃ­tico, 30s para warning
- **repeat_interval**: 5min crÃ­tico, 3h warning, 24h info
- **resolve_timeout**: 5min

---

### 2. **Alert Rules** (`config/alert-rules.yml`)

#### 14 Alertas Implementados

**Grupo 1: Disponibilidade (3 alertas)**
1. âœ… **APIDown**: API fora do ar por 1min â†’ CRITICAL
2. âœ… **DatabaseConnectionPoolExhausted**: Pool > 90% â†’ CRITICAL  
3. âœ… **HighHTTPErrorRate**: > 5% erros 5xx â†’ WARNING

**Grupo 2: Performance (3 alertas)**
4. âœ… **HighLatencyP95**: P95 > 2s por 5min â†’ WARNING
5. âœ… **HighLatencyP99**: P99 > 5s por 3min â†’ CRITICAL
6. âœ… **SlowDatabaseQueries**: Query P95 > 1s â†’ WARNING

**Grupo 3: Recursos (3 alertas)**
7. âœ… **HighCPUUsage**: CPU > 80% por 10min â†’ WARNING
8. âœ… **HighMemoryUsage**: MemÃ³ria > 85% por 5min â†’ WARNING
9. âœ… **DiskSpaceRunningOut**: Disco > 90% â†’ CRITICAL

**Grupo 4: Atendimento (3 alertas)**
10. âœ… **HighTicketQueueSize**: > 50 tickets na fila â†’ WARNING
11. âœ… **SlowTicketResponseTime**: MÃ©dia > 30min â†’ WARNING
12. âœ… **HighTicketAbandonmentRate**: > 15% abandono â†’ CRITICAL

**Grupo 5: SLOs (1 alerta)**
13. âœ… **ErrorBudgetExhausted**: > 80% budget consumido â†’ CRITICAL

**Grupo 6: Business (2 alertas)**
14. âœ… **TrafficDropDetected**: Queda > 50% de trÃ¡fego â†’ WARNING

---

### 3. **SLO Definitions** (`config/slo-definitions.yml`)

#### 7 SLOs Definidos

**SLO 1: Disponibilidade**
- Target: **99.9%** (30 dias)
- Error Budget: **0.1%** = 43min downtime/mÃªs
- Alerta: SLOAvailabilityViolation

**SLO 2: LatÃªncia**
- Target: **P95 < 2s** (7 dias)
- Error Budget: **5%** podem exceder
- Alerta: SLOLatencyViolation

**SLO 3: Taxa de Erros**
- Target: **< 0.1% erros 5xx** (30 dias)
- Error Budget: **0.1%**
- Alerta: HighErrorRate

**SLO 4: Primeira Resposta**
- Target: **P90 < 30min** (7 dias)
- Error Budget: **10%**
- Alerta: SlowFirstResponse

**SLO 5: Tempo de ResoluÃ§Ã£o**
- Target: **P80 < 4h** (30 dias)
- Error Budget: **20%**
- Alerta: SlowResolutionTime

**SLO 6: Database Latency**
- Target: **P95 < 500ms** (1 dia)
- Error Budget: **5%**
- Alerta: SlowDatabaseQueries

**SLO 7: Taxa de ConversÃ£o**
- Target: **> 60%** tickets resolvidos (7 dias)
- Error Budget: **10%**
- Alerta: LowConversionRate

#### Error Budget Policy

| Budget Restante | AÃ§Ã£o | FrequÃªncia Deploy |
|---|---|---|
| > 80% | Normal operations | MÃºltiplos/dia |
| 50-80% | Caution | 1-2/dia |
| 20-50% | Warning | EmergÃªncias apenas |
| < 20% | **FREEZE** | Deploy freeze |

---

### 4. **Runbooks Detalhados**

#### Runbook 1: API Down (`docs/runbooks/api-down.md`)
- âœ… DiagnÃ³stico em 2 minutos
- âœ… 4 soluÃ§Ãµes comuns (processo morto, OOM, DB down, porta ocupada)
- âœ… Checklist de recuperaÃ§Ã£o
- âœ… Procedimento completo (15min)
- âœ… EscalaÃ§Ã£o por tempo
- âœ… RTO: 5 minutos

#### Runbook 2: DB Pool Exhausted (`docs/runbooks/db-pool-exhausted.md`)
- âœ… DiagnÃ³stico SQL detalhado
- âœ… 3 soluÃ§Ãµes imediatas
- âœ… Root Cause Analysis para 4 causas comuns
- âœ… Code review checklist
- âœ… ConfiguraÃ§Ãµes recomendadas
- âœ… PrevenÃ§Ã£o e monitoring

---

## ğŸ“Š Arquitetura de Alerting

```
Prometheus (mÃ©tricas)
    â†“
Alert Rules (avalia a cada 30s)
    â†“
Alertmanager (roteia)
    â†“
    â”œâ”€â”€ Critical â†’ Email + Slack + PagerDuty
    â”œâ”€â”€ Warning â†’ Email + Slack
    â”œâ”€â”€ Info â†’ Slack
    â””â”€â”€ SLO â†’ #slo-violations
```

---

## ğŸš€ Como Usar

### 1. Iniciar Alertmanager

```bash
# Via Docker
docker run -d \
  --name alertmanager \
  -p 9093:9093 \
  -v $(pwd)/backend/config/alertmanager.yml:/etc/alertmanager/alertmanager.yml \
  prom/alertmanager:latest

# Verificar
curl http://localhost:9093/-/healthy
```

### 2. Configurar Prometheus

```yaml
# prometheus.yml
rule_files:
  - 'alert-rules.yml'

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']
```

```bash
# Reload config
curl -X POST http://localhost:9090/-/reload
```

### 3. Configurar VariÃ¡veis de Ambiente

```bash
# .env
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
PAGERDUTY_SERVICE_KEY=your-pagerduty-integration-key
SMTP_USERNAME=alerts@conectcrm.com
SMTP_PASSWORD=your-smtp-password
```

### 4. Testar Alertas

```bash
# Simular alerta crÃ­tico
curl -X POST http://localhost:9093/api/v1/alerts -d '[{
  "labels": {
    "alertname": "APIDown",
    "severity": "critical",
    "instance": "localhost:3001"
  },
  "annotations": {
    "summary": "Teste de alerta crÃ­tico"
  }
}]'

# Ver alertas ativos
curl http://localhost:9093/api/v1/alerts
```

---

## ğŸ“ˆ Dashboards Recomendados

### Dashboard 1: SLO Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Availability (30d): 99.93% âœ…       â”‚
â”‚ Latency P95 (7d): 1.8s âœ…           â”‚
â”‚ Error Rate (1h): 0.05% âœ…           â”‚
â”‚ First Response (7d): 28min âœ…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dashboard 2: Error Budget
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Error Budget Remaining: 65%         â”‚
â”‚ Burn Rate: 0.8x (Normal)            â”‚
â”‚ Days Until Exhaustion: 23d          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dashboard 3: Alertas Ativos
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”´ Critical: 0                       â”‚
â”‚ ğŸŸ¡ Warning: 2 (CPU, Memory)          â”‚
â”‚ ğŸ”µ Info: 1 (Traffic Drop)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Conceitos Importantes

### SLI vs SLO vs SLA

**SLI** (Service Level Indicator)
- MÃ©trica que mede o serviÃ§o
- Exemplo: % de requisiÃ§Ãµes com status 2xx

**SLO** (Service Level Objective)
- Target/meta da SLI
- Exemplo: 99.9% de disponibilidade

**SLA** (Service Level Agreement)
- Contrato com cliente (SLO + consequÃªncias)
- Exemplo: 99.9% ou reembolso de 10%

### Error Budget

**Conceito**: Margem de erro aceitÃ¡vel (1 - SLO)

```
SLO: 99.9% disponibilidade
Error Budget: 0.1% = 43min downtime/mÃªs

Se consumir > 80% do budget:
â†’ Freeze deploys
â†’ Focus em estabilidade
```

### Alert Fatigue Prevention

**Problema**: Muitos alertas â†’ Equipe ignora

**SoluÃ§Ã£o**:
1. âœ… Agrupar alertas relacionados
2. âœ… Inibir warnings quando critical ativo
3. âœ… Repeat interval adequado (nÃ£o spammar)
4. âœ… Severity correta (nÃ£o tudo critical)
5. âœ… Runbooks claros (resolve rÃ¡pido)

---

## ğŸ”— IntegraÃ§Ãµes

### Slack
```bash
# Criar webhook em https://api.slack.com/apps
# Adicionar aos canais:
#alerts-critical
#alerts-warning
#alerts-info
#slo-violations
```

### PagerDuty
```bash
# Criar integration em PagerDuty
# Copiar Integration Key
# Adicionar em alertmanager.yml
```

### Email
```yaml
# Gmail SMTP
smtp_smarthost: 'smtp.gmail.com:587'
smtp_from: 'alerts@conectcrm.com'
smtp_auth_username: 'your-email@gmail.com'
smtp_auth_password: 'app-specific-password'
```

---

## ğŸ“ PrÃ³ximos Passos

### Curto Prazo (Semana 6)
- [ ] Implementar error budget dashboard
- [ ] Criar template de postmortem
- [ ] Configurar on-call rotation
- [ ] Treinar equipe em runbooks

### MÃ©dio Prazo (Semana 7-8)
- [ ] Implementar circuit breaker
- [ ] Adicionar retry automÃ¡tico
- [ ] Health checks avanÃ§ados
- [ ] Chaos engineering tests

### Longo Prazo (Semana 9-12)
- [ ] Multi-region deployment
- [ ] Auto-scaling baseado em SLO
- [ ] Machine learning para anomaly detection
- [ ] Self-healing automation

---

## ğŸ¯ MÃ©tricas de Sucesso

**Objetivos da Semana 5:**
- âœ… 14 alertas configurados e testados
- âœ… 7 SLOs definidos com error budgets
- âœ… 2 runbooks detalhados criados
- âœ… Alertmanager configurado e rodando
- âœ… IntegraÃ§Ã£o Slack/Email funcionando

**KPIs:**
- MTTD (Mean Time To Detect): < 1min
- MTTR (Mean Time To Resolve): < 15min
- Alert Accuracy: > 95%
- False Positive Rate: < 5%

---

## ğŸ“š ReferÃªncias

- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Prometheus Alerting](https://prometheus.io/docs/alerting/latest/overview/)
- [Alertmanager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)
- [SLO Workshop by Google](https://sre.google/workbook/slo-engineering/)

---

**Status**: âœ… **COMPLETO** - Semana 5 implementada com sucesso!  
**PrÃ³ximo**: Semana 6 - Error Budget Management & Postmortems
